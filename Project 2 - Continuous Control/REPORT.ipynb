{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project report: Continuous control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Description of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning historically boils down two kind of algorithms:\n",
    "\n",
    "* **Value based methods (Q-learning):** These algorithms try to find the underlying Q-values, represented by $q^*$. In Deep Q-learning, we use neural networks to approximate the true $q^*$, and the objective is then to find the weights, $w$, of the Q-network which best approximates $q^*$. For a finite set of actions, these algorithms often work well.\n",
    "\n",
    "* **Policy based methods (REINFORCE):** These algorithm directly optimize the policy, without the detour through finding Q-values. \n",
    "\n",
    "Both of these algorithms have their strength and weaknesses. A natural question then is, can we combine the strengths and escape the weaknesses? Well, yeah! Give way for *Actor-critic methods!* This best-of-two worlds approach will\n",
    "\n",
    "* Create a *Critic* which measures how good the action taken is\n",
    "* Create a *Actor* which controls the behaviour of the agent\n",
    "\n",
    "Loosely speaking, we use the actor to take actions, and the critic gives feedback on how good the action was. The actor then learns from the feedback provided from the critic and simultaneously the critic will learn to give better feedback next time based on what happened when the agent took an action. \n",
    "\n",
    "Okay, enough with this hand-waving stuff. Let's explore the mathematical stuff:\n",
    "\n",
    "We will have two networks (local and target) both for the actor and critic, denoted by $\\pi(s,\\theta)$ and $\\pi(s,\\theta')$ for the actor and $\\hat{q}(s,a,w)$ and $\\hat{q}(s,a,w')$ for the critic, where we use ' to distinguish between local and target, respectively.\n",
    "\n",
    "### Learning step\n",
    "The learning procedure goes as follows:\n",
    "\n",
    "1. Sample from ReplayBuffer (See [Project 1](https://github.com/oelvetun/Deep-Reinforcement-Learning/tree/master/Project%201%20-%20Navigation) for more details regarding replay buffer) of BATCH_SIZE\n",
    "\n",
    "2. Get the next actions $a_{t+1} = \\pi(s_{t+1},\\theta')$\n",
    "\n",
    "3. Compute the $Q_{TARGET} = R_t + (\\gamma * \\hat{q}(s_{t+1},a_{t+1},w')$\n",
    "\n",
    "4. Compute the expected Q-function $Q_{EXPECTED} = \\hat{q}(s_t,a_t,w)$\n",
    "\n",
    "5. Measure the error $\\|Q_{TARGET} - Q_{EXPECTED}\\|_{L^2}$ and run backprop to update $w$.\n",
    "\n",
    "6. Get the predicted actions $a_{PRED} = \\pi(s_t,\\theta)$.\n",
    "\n",
    "7. Calculate the actor loss by $-\\frac{1}{n}\\sum{\\hat{q}(s_t,a_{PRED},w)}$ and perform a backprop step.\n",
    "\n",
    "8. Perform a soft update step of the actor and the critic:\n",
    "\n",
    "\\begin{eqnarray} w' &=& \\tau w + (1-\\tau) w' \\nonumber \\\\ \\theta' &=& \\tau \\theta + (1-\\tau) \\theta' \\nonumber \\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Chosen Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ``BUFFER_SIZE = int(5e5)``:  Chosen size of replay buffer\n",
    "* ``BATCH_SIZE = 128``:        Chosen batch size of learning examples \n",
    "* ``GAMMA = 0.99``:            Discount factor\n",
    "* ``TAU = 1e-3``:              Soft update of fixed Q-target weights\n",
    "* ``LR_ACTOR = 2e-4``:         Learning rate of actor in optimization algorithm\n",
    "* ``LR_CRITIC = 3e-4``:        Learning rate of critic in optimization algorithm\n",
    "* ``UPDATE_EVERY = 20``:       Number of actions chosen between each learning step\n",
    "* ``TIMES_UPDATE = 10``:       Number of batches to run each time we update   \n",
    "* ``EPSILON = 1``:             Starting point for noise decline\n",
    "* ``EPSILON_DECAY = 0.005``:   Noise decay for each episode\n",
    "\n",
    "Parameters in Ornstein-Uhlenbeck process\n",
    "* ``MU = 0.0``\n",
    "* ``THETA = 0.15`` \n",
    "* ``SIGMA = 0.2``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actor\n",
    "\n",
    "The neural network we use for the *Actor* is a simple feed-forward network with the following layers\n",
    "\n",
    "* BatchNorm 1\n",
    "* Layer 1: (state_size, 128)\n",
    "* ReLU 1\n",
    "* BatchNorm 2\n",
    "* Layer 2: (128, 128)\n",
    "* ReLU 2\n",
    "* BatchNorm 3\n",
    "* Layer 3: (128, action_size)\n",
    "* Tanh\n",
    "\n",
    "where state_size = 33 and action_size = 4.\n",
    "\n",
    "##### Critic\n",
    "\n",
    "The neural network we use for the *Critic* is a simple feed-forward network with the following layers\n",
    "\n",
    "* Layer 1: (state_size, 256)\n",
    "* ReLU 1\n",
    "* BatchNorm\n",
    "* Layer 2: (cat(128, action_size), 128)\n",
    "* ReLU 2\n",
    "* Layer 3: (128, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm used 39 episodes to solve the problem. We see a plot of the rewards received for each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"scores.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to improve the performance of the agent. Specifically, one could \n",
    "\n",
    "* Try to add noise directly to the parameters, instead of on the action. This have been shown to often give superior performance. The algorithm can be explored [here](https://arxiv.org/abs/1706.01905).\n",
    "* Spend much more effort on tuning the hyperparameters. What would the optimal choice of network architecture? Could we change the learning parameter, or the importance sampling parameters to improve the learning? With more time on our hands, we can spent a lot of time on this. That being said, I have already spent quite some time on this, so we have already come a long way. Be aware that reinforcement learning algorithms are can easily diverge with wrong hyperparameters. Particulary is the DDPG-method applied here relatively unstable.\n",
    "* Consequently, it will be interesting to implement more stable and advanced algorithms, such as Trust Region Policy Optimization (TRPO) or Truncated Natural Policy Gradient (TNPG).\n",
    "* Make the algorithm more realistic by using raw pixel data as input, instead of the sensors on velocity, rotation etc. This will make its observation more identical to human perception. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
