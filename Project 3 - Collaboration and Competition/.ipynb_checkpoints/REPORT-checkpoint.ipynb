{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Project report: Collaboration and Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Description of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, like in the previous, we apply the Deep Deterministic Policy Gradient (DDPG) algorithm to solve the problem.\n",
    "\n",
    "For the details regarding the algorithm, please see the [REPORT](https://github.com/oelvetun/Deep-Reinforcement-Learning/blob/master/Project%202%20-%20Continuous%20Control/REPORT.ipynb) from that project.\n",
    "\n",
    "The only difference is essentially that we here have applied an Experience Replay Buffer, which we previously explained in the [REPORT](https://github.com/oelvetun/Deep-Reinforcement-Learning/blob/master/Project%201%20-%20Navigation/REPORT.ipynb) from the first project.\n",
    "\n",
    "The main parts of the algorithm can be summarized as follows:\n",
    "* A shared (prioritized) replay buffer for both of the agents\n",
    "* Shared network weights for both agents (and also a common critic)\n",
    "* Each agent observes its own local environment and adds the experience to the replay buffer\n",
    "* At each time step, both agents pick an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Chosen Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ``BUFFER_SIZE = int(1e5)``:  Chosen size of replay buffer\n",
    "* ``BATCH_SIZE = 128``:        Chosen batch size of learning examples \n",
    "* ``GAMMA = 0.99``:            Discount factor\n",
    "* ``TAU = 1e-3``:              Soft update of fixed Q-target weights\n",
    "* ``LR_ACTOR = 2e-4``:         Learning rate of actor in optimization algorithm\n",
    "* ``LR_CRITIC = 1e-4``:        Learning rate of critic in optimization algorithm\n",
    "* ``UPDATE_EVERY = 2``:        Number of actions chosen between each learning step\n",
    "* ``TIMES_UPDATE = 1``:        Number of batches to run each time we update   \n",
    "* ``EPSILON = 1``:             Starting point for noise decline\n",
    "* ``EPSILON_DECAY = 1e-4``:    Noise decay for each episode\n",
    "\n",
    "Parameters in Ornstein-Uhlenbeck process\n",
    "* ``MU = 0.0``\n",
    "* ``THETA = 0.15`` \n",
    "* ``SIGMA = 0.2``\n",
    "\n",
    "Parameters for Prioritized Experience Replay\n",
    "* ``ALPHA``: 0.5               Randomness in priority\n",
    "* ``BETA_START``: 0.3          Starting point for importance sampling tuning\n",
    "* ``BETA_INCREASE``: 1e-4      Increase of beta for importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actor\n",
    "\n",
    "The neural network we use for the *Actor* is a simple feed-forward network with the following layers\n",
    "\n",
    "* BatchNorm 1\n",
    "* Layer 1: (state_size, 128)\n",
    "* ReLU 1\n",
    "* BatchNorm 2\n",
    "* Layer 2: (128, 128)\n",
    "* ReLU 2\n",
    "* BatchNorm 3\n",
    "* Layer 3: (128, action_size)\n",
    "* Tanh\n",
    "\n",
    "where state_size = 33 and action_size = 4.\n",
    "\n",
    "##### Critic\n",
    "\n",
    "The neural network we use for the *Critic* is a simple feed-forward network with the following layers\n",
    "\n",
    "* Layer 1: (state_size, 256)\n",
    "* ReLU 1\n",
    "* BatchNorm\n",
    "* Layer 2: (cat(128, action_size), 128)\n",
    "* ReLU 2\n",
    "* Layer 3: (128, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm used 8330 episodes to solve the problem. We see a plot of the rewards received for each episode. After approx 2500 episodes, the goal was almost solved, but then it deteriorated, and only came back after more than 8000 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tennis_rewards.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to improve the performance of the agent. Specifically, one could \n",
    "\n",
    "* Try to add noise directly to the parameters, instead of on the action. This have been shown to often give superior performance. The algorithm can be explored [here](https://arxiv.org/abs/1706.01905).\n",
    "* Spend much more effort on tuning the hyperparameters. What would the optimal choice of network architecture? Could we change the learning parameter, or the importance sampling parameters to improve the learning? With more time on our hands, we can spent a lot of time on this. That being said, I have already spent quite some time on this, so we have already come a long way. Be aware that reinforcement learning algorithms are can easily diverge with wrong hyperparameters. Particulary is the DDPG-method applied here relatively unstable.\n",
    "* Consequently, it will be interesting to implement more stable and advanced algorithms, MADDPG, which is specially designed for such Multi-Agent problems.\n",
    "* With more exploration of the parameters, could it be possible to mirror the states and actions, such that the network can learn from both sides simultaneously. That is, good actions from one side should be able to be mirrored such that the agent knows that this action would also be good to perform at the other side of the net. (It might be that the states is already formulated in this manner, but could be worth investigating)\n",
    "* Make the algorithm more realistic by using raw pixel data as input, instead of the sensors on position and velocity. This will make its observation more identical to human perception. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
