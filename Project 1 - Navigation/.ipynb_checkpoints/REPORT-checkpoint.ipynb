{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project report: Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Description of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, in a Q-learning algorithm, we aim to find the true underlying Q-values, represented by $q^*$. In Deep Q-learning, we use neural networks to approximate the true $q^*$, and the objective is then to find the weights, $w$, of the Q-network which best approximates $q^*$. That is, we want to minimize\n",
    "\n",
    "\\begin{equation} \\min_w (q^*(s,a) - q(s,a;w))^2  \\end{equation}\n",
    "\n",
    "In this model, the value with default parameters is an implementation of the Double Q-Network model with an Experience Buffer and Fixed Q-targets. We will now explain the details of this algorithm.\n",
    "\n",
    "1. We start by creating a Replay Buffer (for Experience Replay) of size BUFFER_SIZE. This means that we will save the last BUFFER_SIZE experiences, which consists of tuples ($s_t$, $a_t$, $r_t$, $s_{t+1}$, $d_t$). Here, \n",
    "    * $s_t$ is the state at step t,\n",
    "    * $a_t$ is the action chosen at step t,\n",
    "    * $r_t$ is the reward received after taking action $a_t$ at state $s_t$,\n",
    "    * $s_{t+1}$ is the next state we ended up in,\n",
    "    * $d_t$ is a boolean parameter, which is True if we ended up in a terminal state.\n",
    "    \n",
    "The reason for applying such a Replay Buffer, is to break the correleation between consequtive (state, action)-pairs. With the Replay Buffer, we can sample randomly from our \"memory\", and hence break this correlation.\n",
    "\n",
    "2. Next, we initialize two neural networks with random weights. Why two networks you ask? Well, remember that we use a network $q(s,a;w)$ to approximate the true underlying Q-values. Therefore, if we only use one network, we would end up with the update \n",
    "\n",
    "\\begin{equation} \\Delta w = \\alpha(r + \\gamma \\max_a q(s',a;w) - q(s,a;w))\\nabla_w q(s,a;w)  , \\end{equation}\n",
    "\n",
    "but this turns out to be very unstable, since the weights we are updating are also present in the TD-target (i.e. $r + \\gamma \\max_a q(s',a;w)$). Therefore, we introduce fixed Q-targets, which means that during learning, we fix the target, $w'$, and consequently get\n",
    "\n",
    "\\begin{equation} \\Delta w = \\alpha(r + \\gamma \\max_a q(s',a;w') - q(s,a;w))\\nabla_w q(s,a;w). \\end{equation}\n",
    "\n",
    "3. After these initializations, we start the iterative process.\n",
    "    * From the given state $s_t$, choose action using an $\\epsilon$-greedy strategy and observe reward $r_t$, the next state $s_{t+1}$ and boolean $d_t$ to see if the episode has ended.\n",
    "    * Store the experience ($s_t$, $a_t$, $r_t$, $s_{t+1}$, $d_t$) in the Replay Buffer.\n",
    "    * If ``t % UPDATE_EVERY = 0``, we draw a sample of BATCH_SIZE from the Replay Buffer, and take an optimization step according to the equation above.\n",
    "    * Update the fixed Q-targets after learning, by the soft update \n",
    "        \\begin{equation} w' = \\tau w + (1-\\tau) w'. \\end{equation}\n",
    "    We continue this process until convergence. In our particular case, the problem is considered solved when the agent obtains an average score above 13 over 100 consequtive episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Chosen Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ``BUFFER_SIZE = int(1e5)``:  Chosen size of replay buffer\n",
    "* ``BATCH_SIZE = 64``:         Chosen batch size of learning examples \n",
    "* ``GAMMA = 0.99``:            Discount factor\n",
    "* ``TAU = 1e-3``:              Soft update of fixed Q-target weights\n",
    "* ``LR = 5e-4``:               Learning rate in optimization algorithm\n",
    "* ``UPDATE_EVERY = 4``:        Number of actions chosen between each learning step\n",
    "* ``DDQN = True``:             We apply a Double Deep Q-Network (see Chapter 2.1)\n",
    "* ``PRB = True``:              We apply a Prioritized ReplayBuffer (see Chapter 2.2)\n",
    "* ``Dueling = True``:          We apply a Dueling Deep Q-Network (see Chapter 2.3)\n",
    "* ``ALPHA = 0.5``:             Randomness in priority when using prioritized experience (for PRB)\n",
    "* ``BETA_START = 0.3``:        Starting point for importance sampling tuning (for PRB)\n",
    "* ``BETA_INCREASE = 1e-4``:    Increase of beta for importance sampling (for PRB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network we use (when ``Dueling = False``) is a simple feed-forward network with the following layers\n",
    "\n",
    "* Layer 1: (state_size, 64)\n",
    "* ReLU 1\n",
    "* Layer 2: (64, 128)\n",
    "* ReLU 2\n",
    "* Layer 3: (128, action_size)\n",
    "\n",
    "where state_size = 37 and action_size = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "In Section 1.2 we explained the Deep Q-Network with Experience Replay and Fixed Q-targets. However, as mentioned in Section 1.3, we apply some extensions to this algorithm, which we will now elaborate a bit more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DQN algorithm has a weakness in the initial stage, where we typically overestimate our actions, since we evaluate the actions with the same network from which we choose the actions. \n",
    "\n",
    "A brilliant remedy to this phenomena, we simply use the local network to choose the action in the TD target, and evaluate using the fixed Q-network weights, i.e.\n",
    "\n",
    "\\begin{equation} R + \\gamma q(s', \\arg \\max_a q(s',a; w); w'), \\end{equation}\n",
    "\n",
    "and that is it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next extension is a bit more involved, requiring some more coding. The idea is to add a priority to each experience stored in the buffer. An intuitive way to prioritize the experiences, is from the TD error. That is, if the error is large, we should assign a higher probability of sampling this again in order to learn this kind of situation better. In order to obtain this, we need the steps\n",
    "\n",
    "* Priority: $p_t = |\\delta_t| + e$ \n",
    "* Sampling probability: $P_t = \\frac{p_t}{\\sum_k p_k}$\n",
    "* Modified update rule: $\\Delta w = \\alpha (N\\cdot P_t)^{\\beta}\\delta_t \\nabla_w q(s_i, a_i;w)$\n",
    "\n",
    "The reason for the modified update rule, is the fact that we no longer sample from a uniform distribution, but from a weighted. The importance weight is implemented in order to take care of this. Typically, we choose $\\beta$ relatively small in the beginning, before it converge towards 1. This is important in order to use the full importance sampling in later stages of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last extension we apply is the Dueling DQN-algorithm. This does not involve any changes to the agent, but to the neural network which approximate the true Q-values. \n",
    "\n",
    "In Section 1.3 we explained the architecture of the default neural net. In a Dueling DQN, we split the net into two streams, the state stream and the advantage stream. This is built on the equation $Q(s,a) = V(s) + A(s,a)$. That is, we realize that the Q-values can be represented as the underlying value of a state and the advantage (slash disadvantage) of taking a certain action in that state. Consequently, we can for several states, where the choice of action is rather irrelevant, learn the value $V(s)$ of the state faster. The general consept is visualized in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/duelingDQN.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the specific architecture is as follows:\n",
    "\n",
    "* Linear 1: (state_size, 64)\n",
    "* ReLU 1\n",
    "* Linear 2: (64, 64)\n",
    "* ReLU 2\n",
    "\n",
    "\n",
    "* Adv_Linear 1: (64, 32)\n",
    "* Adv_ReLU 1\n",
    "* Adv_Linear 2: (32, action_size)\n",
    "\n",
    "\n",
    "* Val_Linear 1: (64, 32)\n",
    "* Val_ReLU 1\n",
    "* Val_Linear 2: (32, 1)\n",
    "\n",
    "\n",
    "* Val + Adv - Adv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm used 1184 episodes to solve the problem. We see a plot of the rewards received for each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/iterations.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to improve the performance of the agent. Specifically, one could \n",
    "\n",
    "* Spend much more effort on tuning the hyperparameters. What would the optimal choice of network architecture? Could we change the learning parameter, or the importance sampling parameters to improve the learning? With more time on our hands, we can spent a lot of time on this.\n",
    "* Make the algorithm more realistic by using raw pixel data as input, instead of the ray-based inputs. This will make its observation more identical to human perception. \n",
    "* There are other algorithmic extensions to the DQN than those precented in Chapter 2. Specifically, it would be interesting to learn how to implement A3C, Distributional DQN and Noisy DQN. However, at the current stage, this is beyond the scope of the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
